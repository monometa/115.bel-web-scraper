name: 115.bel
on:
  schedule:
  - cron: "0 0 2 * 0"
  workflow_dispatch:
jobs:
  web-scraper-process:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - uses: actions/setup-java@v1
      with:
        java-version: '11'
    - uses: vemonet/setup-spark@v1
      with:
        spark-version: '3.2.1'
        hadoop-version: '3.2'
    - run: spark-submit --version
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: run data processing script 
      run: |
        python data_processing_spark.py 
#     - name: run parser
#       run: |
#         python get_data.py
#     - uses: actions/upload-artifact@v2
#       with: 
#         name: parsing_results
#         path: ./data
#         retention-days: 5
#     - uses: shallwefootball/s3-upload-action@master
#       with:
#         aws_key_id: ${{secrets.AWS_KEY_ID}}
#         aws_secret_access_key: ${{secrets.AWS_SECRET_ACCESS_KEY}}
#         aws_bucket: 115bel
#         source_dir: ./data
#         destination_dir: from_parser
